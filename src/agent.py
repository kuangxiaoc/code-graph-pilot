import os
import operator
from typing import TypedDict, Annotated, List, Union
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from neo4j import GraphDatabase
from langgraph.graph import StateGraph, END

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- 1. é…ç½® Qwen æ¨¡å‹ ---
llm = ChatOpenAI(
    model='qwen-plus',
    openai_api_key=os.getenv("QWEN_API_KEY"),
    openai_api_base=os.getenv("QWEN_BASE_URL"),
    temperature=0.1,
    streaming=True
)

# --- 2. Neo4j å›¾æ£€ç´¢å·¥å…· (GraphRAG æ ¸å¿ƒ) ---
def query_dependencies(func_name: str) -> str:
    """
    åœ¨ Neo4j ä¸­æ‰§è¡ŒåŒå‘æŸ¥è¯¢
    """
    uri = os.getenv("NEO4J_URI")
    auth = (os.getenv("NEO4J_USER"), os.getenv("NEO4J_PASSWORD"))
    
    query = """
    MATCH (target:Function {name: $name})
    OPTIONAL MATCH (target)-[:CALLS]->(downstream)
    OPTIONAL MATCH (upstream)-[:CALLS]->(target)
    RETURN 
        target.filepath as filepath,
        target.lineno as lineno,
        collect(DISTINCT downstream.name) as calls,
        collect(DISTINCT upstream.name) as called_by
    """
    
    try:
        with GraphDatabase.driver(uri, auth=auth) as driver:
            result = driver.execute_query(query, name=func_name).records
            
            if not result:
                return f"âš ï¸ è­¦å‘Šï¼šåœ¨çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°å‡½æ•° '{func_name}'ã€‚è¯·æ£€æŸ¥å‡½æ•°åæ˜¯å¦æ­£ç¡®ã€‚"
            
            record = result[0]
            
            # è¿™é‡Œä¿ç•™è‹±æ–‡ Key æ˜¯ä¸ºäº†è®© LLM æ›´å‡†ç¡®åœ°ç†è§£ç»“æ„ï¼Œä½†æˆ‘ä»¬åœ¨ Prompt é‡Œä¼šè¦æ±‚å®ƒç”¨ä¸­æ–‡è§£é‡Š
            context_str = f"""
            [ç›®æ ‡å®ä½“]: {func_name}
            [ä½ç½®]: {record['filepath']} (è¡Œå·: {record.get('lineno', 'æœªçŸ¥')})
            
            [å›¾è°±ä¾èµ–åˆ†æ]
            1. ä¸‹æ¸¸ä¾èµ– (å®ƒè°ƒç”¨äº†è° / Outbound):
               {record['calls'] if record['calls'] else "æ—  (None)"}
               -> ä¿®æ”¹ {func_name} å¯èƒ½ä¼šæ”¹å˜ä¼ ç»™è¿™äº›å‡½æ•°çš„å‚æ•°ã€‚
               
            2. ä¸Šæ¸¸å½±å“ (è°è°ƒç”¨äº†å®ƒ / Inbound):
               {record['called_by'] if record['called_by'] else "æ—  (None - å¯èƒ½æ˜¯å…¥å£å‡½æ•°æˆ–æœªè¢«ä½¿ç”¨)"}
               -> ä¸¥é‡è­¦å‘Š: ä¿®æ”¹ {func_name} å°†ç›´æ¥å¯¼è‡´è¿™äº›è°ƒç”¨è€…å‡ºé”™ã€‚
            """
            return context_str
            
    except Exception as e:
        return f"æ•°æ®åº“è¿æ¥é”™è¯¯: {str(e)}"

# --- 3. å®šä¹‰ Agent çŠ¶æ€ ---
class AgentState(TypedDict):
    query: str
    target_func: str
    context: str
    response: str
    revision_count: int
    feedback: str

# --- 4. èŠ‚ç‚¹å‡½æ•°å®šä¹‰ ---

def parse_intent(state: AgentState):
    """èŠ‚ç‚¹ 1: æ„å›¾è¯†åˆ«"""
    print("--- [Step 1] è§£ææ„å›¾ ---")
    txt = state['query']
    clean_txt = txt.strip().rstrip("?.!")
    words = clean_txt.split()
    target = words[-1] if words else ""
    # å¦‚æœç”¨æˆ·è¾“å…¥åŒ…å«ä¸­æ–‡æ ‡ç‚¹ï¼Œå¯èƒ½éœ€è¦é¢å¤–å¤„ç†ï¼Œè¿™é‡Œç®€å•å¤„ç†
    target = target.replace("ã€‚", "").replace("ï¼Ÿ", "")
    return {"target_func": target, "revision_count": 0}

def retrieve_graph(state: AgentState):
    """èŠ‚ç‚¹ 2: å›¾è°±æ£€ç´¢"""
    print(f"--- [Step 2] æ£€ç´¢å›¾è°±: {state['target_func']} ---")
    context = query_dependencies(state['target_func'])
    return {"context": context}

def generate_answer(state: AgentState):
    """èŠ‚ç‚¹ 3: ç”Ÿæˆå›ç­” (ä¸­æ–‡ç‰ˆ)"""
    print("--- [Step 3] ç”Ÿæˆå›ç­” ---")
    
    # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå°† Prompt æ”¹ä¸ºä¸­æ–‡ ğŸ”¥ğŸ”¥ğŸ”¥
    prompt = f"""
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ Python æ¶æ„å¸ˆå’Œä»£ç é‡æ„ä¸“å®¶ã€‚
    
    ç”¨æˆ·é—®é¢˜: "{state['query']}"
    
    === ä»£ç çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ ===
    {state['context']}
    ===============================
    
    ä¹‹å‰çš„å®¡æŸ¥åé¦ˆ (å¦‚æœæœ‰): {state.get('feedback', 'æ— ')}
    
    ä»»åŠ¡:
    è¯·åŸºäºå›¾è°±ä¸Šä¸‹æ–‡ï¼Œåˆ†æä¿®æ”¹å‡½æ•° '{state['target_func']}' å¸¦æ¥çš„å½±å“ã€‚
    
    è¦æ±‚:
    1. **å¿…é¡»ä½¿ç”¨ä¸­æ–‡å›ç­”**ã€‚
    2. æ˜ç¡®æŒ‡å‡ºé£é™©ç­‰çº§ (ä½/ä¸­/é«˜)ã€‚
    3. åˆ†åˆ«è¯´æ˜å¯¹â€œä¸‹æ¸¸ä¾èµ–â€å’Œâ€œä¸Šæ¸¸è°ƒç”¨è€…â€çš„å½±å“ã€‚
    4. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ˜¾ç¤ºâ€œæ— â€ï¼Œè¯·æ˜ç¡®è¯´æ˜è¯¥å‡½æ•°å¯èƒ½æ˜¯ä¸€ä¸ªå­¤ç«‹å‡½æ•°æˆ–å…¥å£ç‚¹ã€‚
    5. ä¿æŒä¸“ä¸šã€æ¡ç†æ¸…æ™°ï¼Œä½¿ç”¨ Markdown æ ¼å¼ã€‚
    """
    
    response = llm.invoke(prompt)
    return {"response": response.content}

def review_answer(state: AgentState):
    """èŠ‚ç‚¹ 4: ç»“æœå®¡æŸ¥ (ä¸­æ–‡ç‰ˆ)"""
    print("--- [Step 4] å®¡æŸ¥å›ç­” ---")
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä»£ç åŠ©æ‰‹ QA å®¡æŸ¥å‘˜ã€‚
    
    å›¾è°±ä¸Šä¸‹æ–‡: {state['context']}
    ç”Ÿæˆçš„å›ç­”: {state['response']}
    
    è¯·æ£€æŸ¥:
    1. å›ç­”æ˜¯å¦é—æ¼äº†ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®å½±å“ï¼ˆç‰¹åˆ«æ˜¯â€œä¸Šæ¸¸å½±å“/Called Byâ€ï¼‰ï¼Ÿ
    2. å›ç­”æ˜¯å¦äº§ç”Ÿäº†å¹»è§‰ï¼ˆç¼–é€ äº†ä¸å­˜åœ¨çš„ä¾èµ–ï¼‰ï¼Ÿ
    3. **å›ç­”æ˜¯å¦ä½¿ç”¨äº†ä¸­æ–‡ï¼Ÿ**
    
    å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œä»…è¾“å‡º "PASS"ã€‚
    å¦‚æœæœ‰é—®é¢˜ï¼Œè¾“å‡º "FAIL: <å…·ä½“åŸå› >"ã€‚
    """
    
    review = llm.invoke(prompt).content
    print(f"--- å®¡æŸ¥ç»“æœ: {review} ---")
    
    if "PASS" in review:
        return {"feedback": "PASS"}
    else:
        return {"feedback": review, "revision_count": state["revision_count"] + 1}

# --- 5. è·¯ç”±é€»è¾‘ ---

def check_review_outcome(state: AgentState):
    if state["feedback"] == "PASS" or state["revision_count"] >= 1:
        return "end"
    else:
        return "retry"

# --- 6. æ„å»ºå·¥ä½œæµ ---

workflow = StateGraph(AgentState)

workflow.add_node("parse", parse_intent)
workflow.add_node("retrieve", retrieve_graph)
workflow.add_node("generate", generate_answer)
workflow.add_node("review", review_answer)

workflow.set_entry_point("parse")
workflow.add_edge("parse", "retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", "review")

workflow.add_conditional_edges(
    "review",
    check_review_outcome,
    {
        "end": END,
        "retry": "generate"
    }
)
app_graph = workflow.compile()